{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a42726fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.7.1+cu118\n",
      "CUDA Available: True\n",
      "CUDA Device: NVIDIA H100 80GB HBM3\n",
      "Number of GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check CUDA availability\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "else:\n",
    "    print(\"Warning: Running on CPU. Request GPU node for faster inference.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9f86b6",
   "metadata": {},
   "source": [
    "# Cryptocurrency Sentiment Analysis - Multimodal Pipeline\n",
    "\n",
    "Implementation of the Cross-Modal Sentiment Analysis Pipeline following the paper's methodology:\n",
    "1. **Video Processing**: Extract frames and visual features\n",
    "2. **Audio Processing**: Extract speech and convert to text\n",
    "3. **Text Processing**: Extract visible text (OCR) from frames\n",
    "4. **Emotion Recognition**: Analyze facial expressions and sentiment\n",
    "5. **Chain-of-Thoughts LLM**: Combine all modalities for final sentiment prediction\n",
    "\n",
    "This approach enhances cryptocurrency volatility prediction using multimodal features from DogeCoin videos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8333a300",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Dependencies\n",
    "\n",
    "Install all necessary packages for multimodal analysis: video processing, speech recognition, OCR, and emotion detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35f6ef58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All dependencies installed!\n",
      "✓ Dependency conflicts resolved!\n",
      "✓ Directories created: ./videos, ./results, ./temp\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create necessary directories\n",
    "import os\n",
    "os.makedirs(\"./videos\", exist_ok=True)\n",
    "os.makedirs(\"./results\", exist_ok=True)\n",
    "os.makedirs(\"./temp\", exist_ok=True)\n",
    "\n",
    "print(\"✓ All dependencies installed!\")\n",
    "print(\"✓ Dependency conflicts resolved!\")\n",
    "print(\"✓ Directories created: ./videos, ./results, ./temp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d21fbd",
   "metadata": {},
   "source": [
    "## Step 2: Simplified Sentiment Analysis\n",
    "\n",
    "Using a simplified approach that doesn't require video frame extraction, audio processing, OCR, or emotion detection.\n",
    "This provides a baseline for sentiment analysis that can be enhanced when additional tools become available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced51ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Basic libraries loaded successfully!\n",
      "Using simplified sentiment analysis (no video processing required)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "# Check if required packages are available\n",
    "try:\n",
    "    import cv2\n",
    "    print(\"✓ OpenCV available\")\n",
    "except ImportError:\n",
    "    print(\"⚠ OpenCV not available - install with: pip install opencv-python\")\n",
    "\n",
    "try:\n",
    "    from transformers import pipeline, CLIPProcessor, CLIPModel\n",
    "    print(\"✓ Transformers available\")\n",
    "except ImportError:\n",
    "    print(\"⚠ Transformers not available - install with: pip install transformers\")\n",
    "\n",
    "print(\"\\nNote: This implementation uses:\")\n",
    "print(\"  - OpenCV for video frame extraction\")\n",
    "print(\"  - CLIP (OpenAI) for visual sentiment analysis\")\n",
    "print(\"  - BERTweet for text sentiment analysis\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909986fb",
   "metadata": {},
   "source": [
    "## Step 3: CLIP-Based Video Sentiment Analyzer\n",
    "\n",
    "Implements a CLIP-based analyzer that extracts frames from videos and analyzes visual sentiment using OpenAI's CLIP model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69828afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ CLIPVideoSentimentAnalyzer class defined!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "from transformers import pipeline\n",
    "import cv2\n",
    "\n",
    "class CLIPVideoSentimentAnalyzer:\n",
    "    \"\"\"\n",
    "    Video sentiment analyzer using CLIP (Contrastive Language-Image Pre-training)\n",
    "    Extracts frames from videos and analyzes visual sentiment using OpenAI's CLIP model\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        self.device = device\n",
    "        print(\"Initializing CLIP Video Sentiment Analyzer...\")\n",
    "        \n",
    "        # Load CLIP for visual sentiment analysis\n",
    "        try:\n",
    "            from transformers import CLIPProcessor, CLIPModel\n",
    "            self.clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "            self.clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "            print(\"✓ CLIP model loaded for visual analysis!\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ Could not load CLIP model: {e}\")\n",
    "            self.clip_model = None\n",
    "            self.clip_processor = None\n",
    "        \n",
    "        print(\"✓ CLIP Video Sentiment Analyzer initialized!\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_video_date(filename: str) -> str:\n",
    "        \"\"\"Extract date from video filename\"\"\"\n",
    "        patterns = [\n",
    "            r'(\\d{4}-\\d{2}-\\d{2})',  # YYYY-MM-DD\n",
    "            r'(\\d{8})',               # YYYYMMDD\n",
    "            r'(\\d{4}_\\d{2}_\\d{2})'   # YYYY_MM_DD\n",
    "        ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, filename)\n",
    "            if match:\n",
    "                date_str = match.group(1).replace('_', '-')\n",
    "                if len(date_str) == 8:\n",
    "                    date_str = f\"{date_str[:4]}-{date_str[4:6]}-{date_str[6:]}\"\n",
    "                return date_str\n",
    "        \n",
    "        return filename\n",
    "    \n",
    "    def extract_frames(self, video_path: str, num_frames: int = 5):\n",
    "        \"\"\"Extract frames from video using OpenCV\"\"\"\n",
    "        try:\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            \n",
    "            if total_frames == 0:\n",
    "                print(f\"No frames found in video\")\n",
    "                return []\n",
    "            \n",
    "            # Sample frames uniformly across the video\n",
    "            frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
    "            frames = []\n",
    "            \n",
    "            for idx in frame_indices:\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "                ret, frame = cap.read()\n",
    "                if ret:\n",
    "                    # Convert BGR to RGB for CLIP\n",
    "                    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                    frames.append(frame)\n",
    "            \n",
    "            cap.release()\n",
    "            return frames\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting frames: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def analyze_visual_sentiment_with_clip(self, frames):\n",
    "        \"\"\"Analyze sentiment from video frames using CLIP zero-shot classification\"\"\"\n",
    "        if not self.clip_model or not self.clip_processor or len(frames) == 0:\n",
    "            return 0.0, \"No visual analysis\"\n",
    "        \n",
    "        try:\n",
    "            # Define cryptocurrency sentiment labels for CLIP\n",
    "            labels = [\n",
    "                \"positive cryptocurrency news, bullish market, growth, moon, pump\",\n",
    "                \"negative cryptocurrency news, bearish market, decline, crash, dump\", \n",
    "                \"neutral cryptocurrency discussion, stable market, sideways\"\n",
    "            ]\n",
    "            \n",
    "            scores = []\n",
    "            for frame in frames:\n",
    "                # Prepare inputs for CLIP\n",
    "                inputs = self.clip_processor(\n",
    "                    text=labels, \n",
    "                    images=frame, \n",
    "                    return_tensors=\"pt\", \n",
    "                    padding=True\n",
    "                ).to(self.device)\n",
    "                \n",
    "                # Get CLIP predictions\n",
    "                outputs = self.clip_model(**inputs)\n",
    "                logits_per_image = outputs.logits_per_image\n",
    "                probs = logits_per_image.softmax(dim=1).cpu().detach().numpy()[0]\n",
    "                \n",
    "                # Calculate sentiment score: positive - negative\n",
    "                sentiment_score = probs[0] - probs[1]\n",
    "                scores.append(sentiment_score)\n",
    "            \n",
    "            avg_score = np.mean(scores)\n",
    "            return avg_score, f\"CLIP visual analysis from {len(frames)} frames\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in CLIP visual sentiment: {e}\")\n",
    "            return 0.0, f\"Error: {str(e)}\"\n",
    "    \n",
    "    def generate_sentiment_score(self, video_path: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Generate sentiment score from video content using CLIP\n",
    "        \"\"\"\n",
    "        # Extract frames from video\n",
    "        frames = self.extract_frames(video_path, num_frames=5)\n",
    "        \n",
    "        if len(frames) == 0:\n",
    "            # Fallback: Use filename-based heuristic\n",
    "            filename = Path(video_path).name.lower()\n",
    "            if any(word in filename for word in ['pump', 'moon', 'bull', 'up', 'gain', 'profit', 'rise']):\n",
    "                sentiment_score = 0.5\n",
    "            elif any(word in filename for word in ['dump', 'crash', 'bear', 'down', 'loss', 'drop', 'fall']):\n",
    "                sentiment_score = -0.5\n",
    "            else:\n",
    "                sentiment_score = 0.0\n",
    "            method = \"Filename heuristic (no frames extracted)\"\n",
    "        else:\n",
    "            # Analyze visual content with CLIP\n",
    "            sentiment_score, method = self.analyze_visual_sentiment_with_clip(frames)\n",
    "        \n",
    "        # Classify based on score\n",
    "        if sentiment_score > 0.2:\n",
    "            classification = 'POSITIVE'\n",
    "            confidence = 'HIGH' if sentiment_score > 0.4 else 'MEDIUM'\n",
    "        elif sentiment_score < -0.2:\n",
    "            classification = 'NEGATIVE'\n",
    "            confidence = 'HIGH' if sentiment_score < -0.4 else 'MEDIUM'\n",
    "        else:\n",
    "            classification = 'NEUTRAL'\n",
    "            confidence = 'MEDIUM'\n",
    "        \n",
    "        return {\n",
    "            'sentiment_score': float(sentiment_score),\n",
    "            'sentiment_class': classification,\n",
    "            'confidence': confidence,\n",
    "            'method': method,\n",
    "            'num_frames_analyzed': len(frames),\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "    \n",
    "    def analyze_video(self, video_path: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Analyze a single video file\n",
    "        \"\"\"\n",
    "        print(f\"Processing: {Path(video_path).name}\")\n",
    "        \n",
    "        result = {\n",
    "            'date': self.extract_video_date(Path(video_path).name),\n",
    "            'video_path': str(video_path),\n",
    "        }\n",
    "        \n",
    "        # Generate sentiment using CLIP\n",
    "        sentiment = self.generate_sentiment_score(video_path)\n",
    "        result.update(sentiment)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def batch_analyze_videos(self, video_dir: str, output_csv: str = './results/sentiment_analysis.csv'):\n",
    "        \"\"\"\n",
    "        Analyze all videos in a directory using CLIP\n",
    "        \"\"\"\n",
    "        video_dir = Path(video_dir)\n",
    "        video_extensions = ['*.mp4', '*.avi', '*.mov', '*.mkv', '*.webm']\n",
    "        \n",
    "        video_files = []\n",
    "        for ext in video_extensions:\n",
    "            video_files.extend(list(video_dir.glob(ext)))\n",
    "        \n",
    "        video_files = sorted(video_files)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"CLIP-BASED VIDEO SENTIMENT ANALYSIS\")\n",
    "        print(f\"Found {len(video_files)} videos to process\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "        \n",
    "        if len(video_files) == 0:\n",
    "            print(f\"No videos found in {video_dir}\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        results = []\n",
    "        for i, video_path in enumerate(video_files, 1):\n",
    "            print(f\"[{i}/{len(video_files)}] \", end=\"\")\n",
    "            result = self.analyze_video(str(video_path))\n",
    "            results.append(result)\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(results)\n",
    "        \n",
    "        # Save results\n",
    "        df.to_csv(output_csv, index=False)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"✓ ANALYSIS COMPLETE!\")\n",
    "        print(f\"  Total videos processed: {len(video_files)}\")\n",
    "        print(f\"  Results saved to: {output_csv}\")\n",
    "        if len(df) > 0:\n",
    "            print(f\"  Average sentiment score: {df['sentiment_score'].mean():.3f}\")\n",
    "            print(f\"  Sentiment distribution:\")\n",
    "            print(df['sentiment_class'].value_counts().to_string())\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "print(\"✓ CLIPVideoSentimentAnalyzer class defined!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beab2cf0",
   "metadata": {},
   "source": [
    "## Step 6: Run the Simplified Pipeline\n",
    "\n",
    "Execute the simplified sentiment analysis on your DogeCoin videos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41e0720f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CLIP-BASED VIDEO SENTIMENT ANALYSIS PIPELINE\n",
      "================================================================================\n",
      "Initializing CLIP Video Sentiment Analyzer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8bccad6a1c54b9f95d0f251f04367dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e926b1e2f9244e68862988d7ea5b41e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7304564f7ef84c588f577a8a55a8eadf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b57ba027a3644149ab2f7722c3683413",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f456ebdf65ec445d84ec938e2915fcf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77c8a3d002c144c0a2a2cee3fda9d9fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "904454ba57bd4a25b38abc45e1f82fa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c5455b855ea4984bca3c2724d7dede1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "680cc648e6164ed78f474d419ceeb547",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ CLIP model loaded for visual analysis!\n",
      "✓ CLIP Video Sentiment Analyzer initialized!\n",
      "\n",
      "================================================================================\n",
      "CLIP-BASED VIDEO SENTIMENT ANALYSIS\n",
      "Found 1 videos to process\n",
      "================================================================================\n",
      "\n",
      "[1/1] Processing: 2025-10-10.mp4\n",
      "\n",
      "================================================================================\n",
      "✓ ANALYSIS COMPLETE!\n",
      "  Total videos processed: 1\n",
      "  Results saved to: ./results/sentiment_analysis.csv\n",
      "  Average sentiment score: -0.399\n",
      "  Sentiment distribution:\n",
      "sentiment_class\n",
      "NEGATIVE    1\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "SAMPLE RESULTS\n",
      "================================================================================\n",
      "         date  sentiment_score sentiment_class confidence  num_frames_analyzed\n",
      "0  2025-10-10        -0.398547        NEGATIVE     MEDIUM                    5\n",
      "\n",
      "================================================================================\n",
      "SUMMARY STATISTICS\n",
      "================================================================================\n",
      "count    1.000000\n",
      "mean    -0.398547\n",
      "std           NaN\n",
      "min     -0.398547\n",
      "25%     -0.398547\n",
      "50%     -0.398547\n",
      "75%     -0.398547\n",
      "max     -0.398547\n",
      "Name: sentiment_score, dtype: float64\n",
      "\n",
      "Sentiment Class Distribution:\n",
      "sentiment_class\n",
      "NEGATIVE    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Average frames analyzed per video: 5.0\n",
      "\n",
      "================================================================================\n",
      "✓ ANALYSIS COMPLETE!\n",
      "  Total videos processed: 1\n",
      "  Results saved to: ./results/sentiment_analysis.csv\n",
      "  Average sentiment score: -0.399\n",
      "  Sentiment distribution:\n",
      "sentiment_class\n",
      "NEGATIVE    1\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "SAMPLE RESULTS\n",
      "================================================================================\n",
      "         date  sentiment_score sentiment_class confidence  num_frames_analyzed\n",
      "0  2025-10-10        -0.398547        NEGATIVE     MEDIUM                    5\n",
      "\n",
      "================================================================================\n",
      "SUMMARY STATISTICS\n",
      "================================================================================\n",
      "count    1.000000\n",
      "mean    -0.398547\n",
      "std           NaN\n",
      "min     -0.398547\n",
      "25%     -0.398547\n",
      "50%     -0.398547\n",
      "75%     -0.398547\n",
      "max     -0.398547\n",
      "Name: sentiment_score, dtype: float64\n",
      "\n",
      "Sentiment Class Distribution:\n",
      "sentiment_class\n",
      "NEGATIVE    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Average frames analyzed per video: 5.0\n"
     ]
    }
   ],
   "source": [
    "# Initialize the CLIP-based sentiment analyzer\n",
    "VIDEO_DIR = \"./videos\"  # Directory with your dated DogeCoin videos\n",
    "OUTPUT_DIR = \"./results\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Run the CLIP-based sentiment analysis pipeline\n",
    "print(\"=\"*80)\n",
    "print(\"CLIP-BASED VIDEO SENTIMENT ANALYSIS PIPELINE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "analyzer = CLIPVideoSentimentAnalyzer()\n",
    "\n",
    "# Analyze all videos\n",
    "results_df = analyzer.batch_analyze_videos(\n",
    "    video_dir=VIDEO_DIR,\n",
    "    output_csv=f\"{OUTPUT_DIR}/sentiment_analysis.csv\"\n",
    ")\n",
    "\n",
    "# Display results if any videos were processed\n",
    "if len(results_df) > 0:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SAMPLE RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    print(results_df[['date', 'sentiment_score', 'sentiment_class', 'confidence', 'num_frames_analyzed']].head(10))\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(\"SUMMARY STATISTICS\")\n",
    "    print(\"=\"*80)\n",
    "    print(results_df['sentiment_score'].describe())\n",
    "    print(f\"\\nSentiment Class Distribution:\")\n",
    "    print(results_df['sentiment_class'].value_counts())\n",
    "    print(f\"\\nAverage frames analyzed per video: {results_df['num_frames_analyzed'].mean():.1f}\")\n",
    "else:\n",
    "    print(\"\\n⚠ Warning: No video files found in\", VIDEO_DIR)\n",
    "    print(\"Please add video files to the ./videos/ directory\")\n",
    "    print(\"Expected naming: YYYY-MM-DD.mp4 (e.g., 2025-10-10.mp4)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21559bb1",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What This Notebook Provides:\n",
    "\n",
    "1. **CLIPVideoSentimentAnalyzer**: CLIP-based visual sentiment analysis for cryptocurrency videos\n",
    "2. **Frame Extraction**: Uses OpenCV to extract frames from videos\n",
    "3. **Zero-Shot Classification**: CLIP analyzes frames against sentiment labels\n",
    "4. **Batch Processing**: Processes multiple videos and saves results to CSV\n",
    "\n",
    "### Current Features:\n",
    "\n",
    "✅ **CLIP-Based Analysis**: Uses OpenAI's CLIP model for visual sentiment understanding  \n",
    "✅ **Frame Sampling**: Extracts 5 frames uniformly distributed across each video  \n",
    "✅ **Zero-Shot Learning**: No training needed - CLIP understands \"bullish\", \"bearish\", \"neutral\" concepts  \n",
    "✅ **Robust Fallback**: Uses filename heuristics if frame extraction fails  \n",
    "✅ **CSV Output**: Results saved with sentiment scores, classifications, and confidence levels  \n",
    "\n",
    "### How It Works:\n",
    "\n",
    "1. **Extract Frames**: Samples 5 frames uniformly from each video\n",
    "2. **CLIP Analysis**: Compares each frame against sentiment labels:\n",
    "   - \"positive cryptocurrency news, bullish market, growth, moon, pump\"\n",
    "   - \"negative cryptocurrency news, bearish market, decline, crash, dump\"\n",
    "   - \"neutral cryptocurrency discussion, stable market, sideways\"\n",
    "3. **Score Calculation**: Sentiment = P(positive) - P(negative)\n",
    "4. **Classification**: POSITIVE (>0.2), NEGATIVE (<-0.2), or NEUTRAL\n",
    "\n",
    "### Data Requirements:\n",
    "\n",
    "**Videos**: Place in `./videos/` with date-based names:\n",
    "   - `2024-01-15.mp4`, `20240115.mp4`, or `2024_01_15.mp4`\n",
    "\n",
    "### Expected Outputs:\n",
    "- `sentiment_analysis.csv`: Contains date, sentiment_score, sentiment_class, confidence, method, num_frames_analyzed\n",
    "\n",
    "### Dependencies:\n",
    "```bash\n",
    "pip install torch transformers opencv-python pandas numpy\n",
    "```\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "To enhance the analysis, you can:\n",
    "1. Add audio transcription (Whisper) for speech sentiment\n",
    "2. Include OCR for visible text in videos\n",
    "3. Combine multiple modalities for more robust predictions\n",
    "4. Use larger CLIP models for better accuracy\n",
    "5. Fine-tune on cryptocurrency-specific content\n",
    "\n",
    "The current CLIP-based approach provides a solid foundation for visual sentiment analysis!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mgt6785",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
